package com.example.pocketscholar.data

/**
 * Quantization seviyesi â€” 4-bit daha az RAM kullanÄ±r, 8-bit daha kaliteli sonuÃ§ verir.
 */
enum class QuantizationMode(val label: String, val badge: String) {
    Q4_K_M("4-bit", "Q4"),
    Q8_0("8-bit", "Q8")
}

/**
 * Uygulama iÃ§inde indirilebilecek GGUF model bilgisi.
 */
data class ModelInfo(
    val id: String,
    val name: String,
    val description: String,
    val sizeLabel: String,       // KullanÄ±cÄ±ya gÃ¶sterilecek boyut (Ã¶r. "1.8 GB")
    val sizeInBytes: Long,       // YaklaÅŸÄ±k boyut (progress hesabÄ± iÃ§in)
    val downloadUrl: String,     // Hugging Face resolve/main URL
    val huggingFaceUrl: String,  // Hugging Face model sayfasÄ± (kullanÄ±cÄ±ya gÃ¶sterilecek)
    val fileName: String,        // Kaydedilecek dosya adÄ±
    val ramRequirement: String,  // Ã–nerilen minimum RAM
    val tier: ModelTier,         // Cihaz seviyesi
    val quantization: QuantizationMode = QuantizationMode.Q4_K_M,  // Quantization seviyesi
    val baseModelId: String? = null  // Q8 varyantÄ± ise, Q4 karÅŸÄ±lÄ±ÄŸÄ±nÄ±n ID'si (gruplama iÃ§in)
)

/**
 * Model seviyesi â€” kullanÄ±cÄ±ya cihazÄ±na uygun modeli seÃ§mesini kolaylaÅŸtÄ±rÄ±r.
 */
enum class ModelTier(val label: String, val emoji: String) {
    LIGHTWEIGHT("Hafif", "âš¡"),
    BALANCED("Dengeli", "âš–ï¸"),
    POWERFUL("GÃ¼Ã§lÃ¼", "\uD83D\uDE80"),
    ADVANCED("Ä°leri Seviye", "ğŸ§ ")
}

/**
 * VarsayÄ±lan model listesi â€” tÃ¼m modeller Hugging Face'den indirilir.
 * Her model iÃ§in Q4_K_M (4-bit) ve Q8_0 (8-bit) varyantlarÄ± mevcut.
 */
object AvailableModels {
    val list: List<ModelInfo> = listOf(
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // Hafif (dÃ¼ÅŸÃ¼k RAM cihazlar, 3-4 GB RAM)
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        // â”€â”€ Qwen 2.5 (0.5B) â”€â”€
        ModelInfo(
            id = "qwen2.5-0.5b",
            name = "Qwen 2.5 (0.5B)",
            description = "En kÃ¼Ã§Ã¼k model. Eski/dÃ¼ÅŸÃ¼k RAM cihazlarda Ã§alÄ±ÅŸÄ±r. Basit sorular iÃ§in uygundur.",
            sizeLabel = "400 MB",
            sizeInBytes = 420_000_000L,
            downloadUrl = "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf",
            huggingFaceUrl = "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF",
            fileName = "qwen2.5-0.5b-instruct-q4_k_m.gguf",
            ramRequirement = "2 GB RAM",
            tier = ModelTier.LIGHTWEIGHT,
            quantization = QuantizationMode.Q4_K_M
        ),
        ModelInfo(
            id = "qwen2.5-0.5b-q8",
            name = "Qwen 2.5 (0.5B) Q8",
            description = "En kÃ¼Ã§Ã¼k model, yÃ¼ksek kalite quantization. Daha iyi doÄŸruluk, biraz daha fazla RAM.",
            sizeLabel = "530 MB",
            sizeInBytes = 530_000_000L,
            downloadUrl = "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q8_0.gguf",
            huggingFaceUrl = "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF",
            fileName = "qwen2.5-0.5b-instruct-q8_0.gguf",
            ramRequirement = "3 GB RAM",
            tier = ModelTier.LIGHTWEIGHT,
            quantization = QuantizationMode.Q8_0,
            baseModelId = "qwen2.5-0.5b"
        ),

        // â”€â”€ TinyLlama (1.1B) â”€â”€
        ModelInfo(
            id = "tinyllama-1.1b",
            name = "TinyLlama (1.1B)",
            description = "Ã‡ok hafif ve hÄ±zlÄ±. DÃ¼ÅŸÃ¼k RAM'li cihazlarda sorunsuz Ã§alÄ±ÅŸÄ±r.",
            sizeLabel = "670 MB",
            sizeInBytes = 670_000_000L,
            downloadUrl = "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            huggingFaceUrl = "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
            fileName = "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            ramRequirement = "3 GB RAM",
            tier = ModelTier.LIGHTWEIGHT,
            quantization = QuantizationMode.Q4_K_M
        ),
        ModelInfo(
            id = "tinyllama-1.1b-q8",
            name = "TinyLlama (1.1B) Q8",
            description = "Ã‡ok hafif ve hÄ±zlÄ±, yÃ¼ksek kalite quantization. Daha iyi doÄŸruluk.",
            sizeLabel = "1.1 GB",
            sizeInBytes = 1_100_000_000L,
            downloadUrl = "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf",
            huggingFaceUrl = "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
            fileName = "tinyllama-1.1b-chat-v1.0.Q8_0.gguf",
            ramRequirement = "4 GB RAM",
            tier = ModelTier.LIGHTWEIGHT,
            quantization = QuantizationMode.Q8_0,
            baseModelId = "tinyllama-1.1b"
        ),

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // Dengeli (orta seviye cihazlar, 4-6 GB RAM)
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        // â”€â”€ SmolLM2 (1.7B) â”€â”€
        ModelInfo(
            id = "smollm2-1.7b",
            name = "SmolLM2 (1.7B)",
            description = "Dengeli performans. Orta seviye cihazlarda iyi sonuÃ§lar verir.",
            sizeLabel = "1.0 GB",
            sizeInBytes = 1_060_000_000L,
            downloadUrl = "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF/resolve/main/smollm2-1.7b-instruct-q4_k_m.gguf",
            huggingFaceUrl = "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF",
            fileName = "smollm2-1.7b-instruct-q4_k_m.gguf",
            ramRequirement = "4 GB RAM",
            tier = ModelTier.BALANCED,
            quantization = QuantizationMode.Q4_K_M
        ),
        ModelInfo(
            id = "smollm2-1.7b-q8",
            name = "SmolLM2 (1.7B) Q8",
            description = "Dengeli performans, yÃ¼ksek kalite quantization. Daha iyi doÄŸruluk.",
            sizeLabel = "1.8 GB",
            sizeInBytes = 1_800_000_000L,
            downloadUrl = "https://huggingface.co/bartowski/SmolLM2-1.7B-Instruct-GGUF/resolve/main/SmolLM2-1.7B-Instruct-Q8_0.gguf",
            huggingFaceUrl = "https://huggingface.co/bartowski/SmolLM2-1.7B-Instruct-GGUF",
            fileName = "SmolLM2-1.7B-Instruct-Q8_0.gguf",
            ramRequirement = "6 GB RAM",
            tier = ModelTier.BALANCED,
            quantization = QuantizationMode.Q8_0,
            baseModelId = "smollm2-1.7b"
        ),

        // â”€â”€ Gemma 2 (2B) â”€â”€
        ModelInfo(
            id = "gemma-2-2b",
            name = "Gemma 2 (2B)",
            description = "Google'Ä±n hafif modeli. Ä°yi TÃ¼rkÃ§e desteÄŸi, dengeli sonuÃ§lar.",
            sizeLabel = "1.7 GB",
            sizeInBytes = 1_710_000_000L,
            downloadUrl = "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf",
            huggingFaceUrl = "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF",
            fileName = "gemma-2-2b-it-Q4_K_M.gguf",
            ramRequirement = "4 GB RAM",
            tier = ModelTier.BALANCED,
            quantization = QuantizationMode.Q4_K_M
        ),
        ModelInfo(
            id = "gemma-2-2b-q8",
            name = "Gemma 2 (2B) Q8",
            description = "Google'Ä±n hafif modeli, yÃ¼ksek kalite quantization. Daha iyi TÃ¼rkÃ§e desteÄŸi.",
            sizeLabel = "2.7 GB",
            sizeInBytes = 2_700_000_000L,
            downloadUrl = "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q8_0.gguf",
            huggingFaceUrl = "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF",
            fileName = "gemma-2-2b-it-Q8_0.gguf",
            ramRequirement = "6 GB RAM",
            tier = ModelTier.BALANCED,
            quantization = QuantizationMode.Q8_0,
            baseModelId = "gemma-2-2b"
        ),

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // GÃ¼Ã§lÃ¼ (iyi cihazlar, 6-8 GB RAM)
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        // â”€â”€ Llama 3.2 (3B) â”€â”€
        ModelInfo(
            id = "llama-3.2-3b",
            name = "Llama 3.2 (3B) â­",
            description = "Ã–nerilen model! En iyi doÄŸruluk/hÄ±z dengesi. Ã‡oÄŸu modern cihazda sorunsuz Ã§alÄ±ÅŸÄ±r.",
            sizeLabel = "1.9 GB",
            sizeInBytes = 2_020_000_000L,
            downloadUrl = "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            huggingFaceUrl = "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF",
            fileName = "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            ramRequirement = "6 GB RAM",
            tier = ModelTier.POWERFUL,
            quantization = QuantizationMode.Q4_K_M
        ),
        ModelInfo(
            id = "llama-3.2-3b-q8",
            name = "Llama 3.2 (3B) Q8 â­",
            description = "Ã–nerilen model, yÃ¼ksek kalite quantization! En iyi doÄŸruluk, daha fazla RAM gerektirir.",
            sizeLabel = "3.4 GB",
            sizeInBytes = 3_420_000_000L,
            downloadUrl = "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf",
            huggingFaceUrl = "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF",
            fileName = "Llama-3.2-3B-Instruct-Q8_0.gguf",
            ramRequirement = "8 GB RAM",
            tier = ModelTier.POWERFUL,
            quantization = QuantizationMode.Q8_0,
            baseModelId = "llama-3.2-3b"
        ),

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // Ä°leri Seviye (flagship cihazlar, 8+ GB RAM)
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        // â”€â”€ Phi 3.5 Mini (3.8B) â”€â”€
        ModelInfo(
            id = "phi-3.5-mini",
            name = "Phi 3.5 Mini (3.8B)",
            description = "Microsoft'un gÃ¼Ã§lÃ¼ modeli. YÃ¼ksek doÄŸruluk ama daha fazla RAM gerektirir.",
            sizeLabel = "2.4 GB",
            sizeInBytes = 2_390_000_000L,
            downloadUrl = "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q4_K_M.gguf",
            huggingFaceUrl = "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF",
            fileName = "Phi-3.5-mini-instruct-Q4_K_M.gguf",
            ramRequirement = "8 GB RAM",
            tier = ModelTier.ADVANCED,
            quantization = QuantizationMode.Q4_K_M
        ),
        ModelInfo(
            id = "phi-3.5-mini-q8",
            name = "Phi 3.5 Mini (3.8B) Q8",
            description = "Microsoft'un gÃ¼Ã§lÃ¼ modeli, yÃ¼ksek kalite quantization. En yÃ¼ksek doÄŸruluk.",
            sizeLabel = "4.0 GB",
            sizeInBytes = 4_060_000_000L,
            downloadUrl = "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q8_0.gguf",
            huggingFaceUrl = "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF",
            fileName = "Phi-3.5-mini-instruct-Q8_0.gguf",
            ramRequirement = "12 GB RAM",
            tier = ModelTier.ADVANCED,
            quantization = QuantizationMode.Q8_0,
            baseModelId = "phi-3.5-mini"
        )
    )

    /** Sadece Q4 (4-bit) modelleri dÃ¶ner */
    fun q4Models(): List<ModelInfo> = list.filter { it.quantization == QuantizationMode.Q4_K_M }

    /** Sadece Q8 (8-bit) modelleri dÃ¶ner */
    fun q8Models(): List<ModelInfo> = list.filter { it.quantization == QuantizationMode.Q8_0 }

    /** Belirli bir modelin Q4 â†” Q8 karÅŸÄ±lÄ±ÄŸÄ±nÄ± bul */
    fun findCounterpart(modelId: String): ModelInfo? {
        val model = list.find { it.id == modelId } ?: return null
        return if (model.quantization == QuantizationMode.Q4_K_M) {
            // Q4 â†’ Q8 karÅŸÄ±lÄ±ÄŸÄ±nÄ± bul
            list.find { it.baseModelId == model.id }
        } else {
            // Q8 â†’ Q4 karÅŸÄ±lÄ±ÄŸÄ±nÄ± bul
            list.find { it.id == model.baseModelId }
        }
    }
}
